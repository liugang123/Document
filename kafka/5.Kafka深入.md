- ## Kafka如何进行复制

- #### 1.broker集群成员关系
```
1.Kafka使用Zookeeper来维护集群成员的信息
2.每个broker都有一个唯一标识符，这个标识符可以在配置文件中指定，也可以自动生成
3.在broker启动时，它通过创建临时节点把自己的ID注册到Zookeeper，Kafka组件订阅Zookeeper的/brokers/ids路径，
  当有broker加入集群或退出集群时，这些组件就可以获得通知
4.如果启动一个具有相同ID的broker，会得到一个错误，新的broker试着进行注册，但不会成功，因为Zookeeper已经具有相同ID的broker
5.在broker停机、出现网络分区或长时间垃圾回收停顿时，broker会从Zookeeper上断开连接，此时broker在启动时创建的临时节点会从Zookeeper上移除，
  监听broker列表的Kafka组件会被告知该broker已移除
6.在关闭broker时，它对应的节点会消失，不过ID会继续存在于其他数据结构中，如主题的副本列表就可能包含这些ID，
  在完全关闭broker之后，如果使用相同的ID启动另一个全新的broker，它会立即加入集群，并拥有与旧broker相同的分区和主题
```

- #### 2.控制器
```
1.控制器在集群中只能有一个，本质上也是一个broker，除了具有一般broker的功能外，还负责分区首领的选举
2.集群中每个启动的broker都会通过Zookeeper创建一个临时节点/controller让自己成为控制器，只有第一个启动的broker会创建成功，其他会收到“节点已存在”的异常
3.其他启动的broker节点在控制器节点上创建Zookeeper watch对象，这样就可以收到这个节点的变更通知
4.如果控制器被关闭或者与Zookeeper断开连接，Zookeeper上的临时节点就会消失，集群里的其他broker通过watch对象得到控制器节点消失的通知，
  就会尝试让自己成为新的控制器
5.每个新选出的控制器通过Zookeeper的条件递增操作获得一个全新的、数值更大的controller epoch，
  其他broker在知道当前的controller epoch后，如果收到由控制器发出的包含较旧的epoch消息，就会忽略它们
6.当控制器发现一个broker已经离开集群，如果分区的首领刚好在这个broker上，控制器就知道这些失去首领的分区需要一个新首领
7.控制器遍历这些分区，并确定谁应该成为新首领，然后向所有包含新首领或现有跟随者的broker发送请求，该请求消息包含谁是新首领以及谁是分区跟随者的信息，
  随后，新首领开始处理来自生产者和消费者的请求，而跟随者开始从新首领哪里复制消息
8.当控制器发现一个broker加入集群时，会使用broker ID检查新加入的broker是否包含现有分区的副本，
  如果有，控制器就把变更通知发送给新加入的broker和其他broker，新broker上的副本开始从分区首领那里复制消息
9.Kafka使用Zookeeper的临时节点来选举控制器，并在节点加入或退出集群时通知控制器，
  控制器负责在节点加入或离开集群时进行分区首领选举
  控制器使用epoch来避免“脑裂”，脑裂是指两个节点同时认为自己是当前的控制器
```

- #### 3.复制
```
1.复制功能是Kafka架构的核心，复制之所以关键，是因为它可以在个别节点失效时仍能保证Kakfa的可用性和持久性
2.Kafka使用主题来组织数据，每个主题分为若干个分区，每个分区有多个副本，
  副本被保存在broker上，每个broker可以保存成百上千个属于不同主题和分区的副本
3.副本有两种类型：
  * 首领副本
    每个分区都有一个首领副本，为了保证一致性，所有生产者请求和消费者请求都会经过这个副本
  * 跟随者副本
    首领以外的副本都是跟随者副本，跟随者副本不处理来自客户端的请求，它们的唯一任务就是从首领副本复制消息，保持与首领一致的状态。
    如果首领发生奔溃，其中的一个跟随者会被提升为新首领
4.首领的另一个任务是确定哪些跟随者的状态与自己是一致的：
  * 跟随者为了保持与首领的状态一致，在新的消息到达时尝试从首领哪里复制消息，不过有各种原因会导致消息同步失败，
    如网络拥塞导致复制变慢，broker发生奔溃导致复制滞后，直到重启broker后复制才会继续
  * 为了与首领保持同步，跟随者向首领发送获取数据的请求，这种请求与消费者为了读取消息发送的请求是一样的，
    首领将响应的消息发给跟随者，请求消息包含了跟随者想获取消息的偏移量，而且这些偏移量总是有序的
  * 通过查看每个跟随者请求的最新偏移量，首领就会知道每个跟随者复制的进度
  * 如果跟随者在10s内没有请求任何消息，或者虽然在请求消息，但在10s内没有请求最新的数据，它就会被认为是不同步的
  * 如果一个副本无法与首领保持一致，在首领失效时，它就不可能成为新首领
  * 持续请求得到最新消息副本被称为同步副本，在首领发生失效时，只有同步副本才能被选为新首领
  * 跟随者的正常不活跃时间或成为不同步副本之前的时间是通过replica.lag.time.max.ms参数来设置的，
    这个时间间隔直接影响着首领选举期间的客户端行为和数据保留机制
5.除了当前首领之外，每个分区都有一个首选首领，创建主题选定的首领就是分区的首选首领，
  创建分区时，需要在broker之间均衡首领，一般希望首选首领成为真正的分区首领
6.默认情况下，Kafka的auto.leader.rebalance.enable被设置为true，就会检查首选首领是不是当前首领，如果不是，并且该副本是同步的，
  那么就会触发首领选举，让首选首领成为当前首领
7.从分区的副本清单里可以容易的找到首选首领，清单里的第一个副本一般就是首选首领，
  要确保首选首领被传播到其他broker上，避免让包含了首领的broker负载过重，而其他broker却无法分担负载
```

- ## Kafka如何处理来自生产者和消费者的请求

- #### 1.处理请求
```
1.broker的大部分工作是处理客户端、分区副本和控制器发送给分区首领的请求
2.kafka提供了一个二进制协议(基于TCP)，指定了请求消息的格式以及broker如何对请求做出响应，包括成功处理请求或在处理请求过程中遇到错误
3.客户端发起连接并发送请求，broker处理请求并响应，broker按照请求到达的顺序来处理它们，
  这种顺序保证让Kafka具有消息队列的特性，同时保证保存的消息也是有序的
4.所有的请求消息都包含一个标准消息头：
  * Request type：即API key
  * Request version：broker可以处理不同版本的客户端请求，并根据客户端版本做出不同的响应
  * Correlation ID：一个具有唯一性的数字，用于标示请求消息，同时也会出现在响应消息和错误日志里，用于诊断问题
  * Client ID：用于标识发送请求的客户端
5.broker处理请求的流程：
  * broker会在它所监听的每个端口上运行一个Acceptor线程，这个线程会创建一个连接，并把它交给Processor线程去处理
  * processor线程，也称网络线程，数量是可以配置的
    负责从客户端获取请求信息，把它们放进请求队列，然后从响应队列获取响应消息，把它们发送给客户端
  * 请求消息被放到请求队列后，IO线程会负责处理它们
6.常见的请求类型：
  * 生产请求： 
    生产者发送的请求，包含客户端要写入broker的消息
  * 获取请求
    在消费者和跟随者副本需要从broker读取消息时发送的请求
7.生产请求和获取请求都必须发送给分区的首领副本，如果broker收到一个针对特定分区的请求，而改分区的首领在另一个broker上，
  那么发送请求的客户端会收到一个“非分区首领”的错误响应
8.Kafka的客户端要自己负责把生产请求和获取请求发送到正确的broker上，客户端发送元数据请求，获取自己关心的主题列表，
  服务器端的响应消息指明了这些主题包含的分区、每个分区都有哪些副本，以及那个副本是首领
9.元数据请求可以发送给任意的broker，因为所有的broker都缓存了这些信息
10.一般情况下，客户端也会缓存请求的元数据信息，并直接往目标broker上发送生产请求和获取请求
11.客户端会时不时的发送元数据请求来刷新元数据信息，刷新的时间间隔可以通过metadata.max.age.ms参数来配置
12.如果客户端收到“非首领”错误，就会尝试重发请求之前先刷新元数据，因为这个错误说明正在使用的元数据已经过期了
```

- #### 2.生产请求
```
1.acks参数指定了需要多少个broker确认才可以认为一个消息写入成功：
  * acks=1：只要首领收到消息就认为写入成功
  * acks=all：需要所有同步副本收到消息才算写入成功
  * acks=0：生产者把消息发出去之后，完全不需要等待broker的响应
2.包含首领副本的broker在收到生产者请求时，会对请求做一些验证：
  * 发送数据的用户是否有主题写入权限
  * 请求里包含的acks值是否有效（0，1，all）
  * 如果acks=all，是否有足够多的同步副本保证消息已经被安全写入
3.消息被写入本地磁盘，在Linux系统上，消息会被写到文件系统缓存里，并不保证它们何时会被刷新到磁盘上，
  kafka不会一致等待数据被写到磁盘上，它依赖复制功能来保证消息的持久性
4.在消息被写入分区的首领之后，broker开始检查acks配置参数：
  * 如果acks为0或1，那么broker立即返回响应
  * 如果acks为all，那么请求会被保存在一个叫作炼狱饿的缓冲区里，直到首领发现所有跟随者副本都复制了消息，响应才会被返回给客户端
```

- #### 3.获取请求
```
1.客户端发送请求，向broker请求主题分区里具有特定偏移量的消息
2.客户端还可以指定broker最多可以从一个分区返回多少数据，客户端需要为broker返回的数据分配足够的内存，
  如果没有这个限制，broker返回的大量数据有可能耗尽客户端的内存
3.请求需要先到达指定的分区首领上，客户端通过查询元数据来确保请求的路由是正确的
4.首领收到请求时，会先检查请求是否有效，如指定的偏移量在分区上是否存在，如果客户端请求的是已经删除的数据，否则，broker将返回一个错误
5.Kafka使用零复制技术向客户端发送消息，Kafka直接把消息从Linux文件系统缓存里发送到网络通道，而不需要经过任何中间缓存区，
  这样避免里字节复制，也不需要管理内存缓冲区，从而获得更好的性能
6.客户端除了可以设置broker返回数据的上限，也可以设置下限，在主题消息流量不是很大的情况下，可以减少CPU和网络开销
7.如果没有足够的数据，客户端也不会一直等待下去，客户端可以定义一个超时时间，如果broker无法在X毫秒内累积满足要求的数据量，
  就把当前的这些数据返回给客户端
8.并不是所有保存在分区首领上的数据都可以被客户端读取，大部分客户端只能读取已经写入所有同步副本的消息
9.分区首领知道每个消息会被复制到那个副本上，在消息还没有被写入所有副本之前，是不会发送给消费者的，
  尝试获取这些消息的请求会得到空的响应而不是错误
10.如果broker间的消息复制因为某些原因变慢，那么到达消费者的时间也随之变长，
   延迟时间可以通过参数replica.lag.time.max.ms来设置，它指定了副本在复制消息时可被允许的最大延迟时间
```

- #### 4.其他请求
```
1.broker之间也可以使用通用的二进制协议来进行通信，这些请求发生在Kafka内部，客户端不应该使用这些请求
2.当一个新首领被选举出来，控制器会发送LeaderAndIsc请求给新首领（开始接收来自客户端请求）和跟随者（开始跟随新首领）
3.新版本的kafka不在使用Zookeeper来保存偏移量，而是把偏移量保存在特定的Kafka主题上，Kafka为其新增加了几种请求类型：
  OffsetCommitRequest、OffsetFetchRequest和ListOffsetRequest
```

- ## Kafka的存储细节

```
1.Kafka的基本存储单元是分区，分区无法在多个broker间进行再细分，也无法在同一个broker的多个磁盘上进行再细分
2.分区的大小受到单个挂载点可用空间的限制
3.在配置Kafka时，log.dirs指定了用于存储分区的目录清单
```

- #### 1.分区分配
```
1.在创建主题时，Kafka首先会决定如何在broker上分配分区，在进行分区分配时，要达到如下目标：
  * 在broker间平均的分布分区副本
  * 确保每个分区的每个副本分布在不同的broker上
  * 如果broker指定了机架信息，尽可能把每个分区的副本分配到不同机架的broker上
2.为分区和副本选好合适的broker之后，就要决定这些分区该使用那个目录，规则很简单：
  * 计算每个目录里分区的数量，新的分区总是被添加到数量最小的那个目录
  * 如果添加了一个新磁盘，所有新的分区都会被创建到这个磁盘，因为完成分配工作之前，新磁盘的分区数量总是最少的
```

- #### 2.文件管理
```
1.保留数据是Kafka的一个基本特性，Kafka不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息
2.kafka管理员为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，
  或者清理数据之前可以保留的数据量大小
3.在一个大文件中查找和删除消息是很耗时的，也容易出错，Kafka把分区分成若干个片段，
  默认情况下，每个片段包含1GB或一周数据，看那个参数先满足
4.在broker往分区写入数据时，如果达到片段上限，就关闭当前文件，并打开一个新的文件
5.当前正在写入数据的片段叫做活跃片段，活跃片段永远不会被删除
```

- #### 3.文件格式
```
1.Kafka把消息和偏移量保存在文件里，保存在磁盘上的数据格式与从生产者发送过来或者发送给消费者的消息格式是一致的
2.使用相同大消息格式进行磁盘存储和网络传输，Kafka可以使用零复制技术给消费者发送消息，
  同时避免了对生产者已压缩过的消息进行解压和再压缩
3.除了键、值和偏移量，消息还包含里消息大小、校验和、消息格式版本号、压缩算法和时间戳，
  时间戳可以是生产者发送消息的时间，也可以是消息到达broker的时间，可以进行配置
4.如果生产者发送的是压缩过的消息，同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送，
  消费者在解压这个消息之后，会看到整个批次的消息，每个消息都有自己的时间戳和偏移量
5.Kafka附带了DumpLogSegment的工具，可以查看文件片段的内容，可以显示每个消息的偏移量、校验和、魔术数字节、消息大小和压缩算法  
```

- #### 4.索引
```
1.消费者可以从Kafka的任意可用偏移量位置开始读取消息，为了让broker快速的定位指定的偏移量，kafka为每个分区维护了一个索引
2.索引把偏移量映射到片段文件和偏移量在文件里的位置
3.索引也被分成片段，所以在删除消息时，也可以删除相应的索引
4.Kafka不维护索引的校验和，如果索引出现损坏，Kafka会通过重新读取消息并录制偏移量和位置来重新生成索引
```