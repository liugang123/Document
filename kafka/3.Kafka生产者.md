- #### 生产者概念
```
1.应用程序在很多情况下需要往Kafka写入消息，不同的使用场景对生产者API的使用和配置会有直接的影响
2.ProducerRecord对象：需要包含目标主题和要发送的内容，还可以指定键和分区
3.序列化器：在发送ProducerRecord对象时，生产者要先把键和值对象序列化成字节数组，才能够在网络上传输
3.分区器：数据被传给分区器，如果之前在ProducerRecord对象中指定了分区，分区器就不在做任何事，直接把指定的分区返回，
  如果没有指定分区，分区器会根据ProducerRecord对象的键来选择一个分区
4.消息批次：指定好分区的消息会被添加到一个记录批次中，批次里的所有消息会被发送到相同的主题和分区上，有一个独立的线程负责把这些记录批次发送到相应的broker上
5.RecordMetaData对象：服务器在收到消息时会返回一个响应，如果消息成功写入Kafka，就返回一个RecordMataData对象，包含主题和分区信息，以及记录在分区里的偏移量，
 如果写入失败，则返回一个错误，生产者收到错误之后会进行重试，几次失败之后如果还是失败，就返回错误信息
```

- #### 创建Kafka生产者
```
1.往Kafka写入消息，要创建一个生产者对象，并设置一些属性，Kafka生产者有3个必选的属性
 * bootstrap.servers
   - 该属性指定broker的地址清单，地址的格式为host:port
   - 清单里不需要包含所有的broker地址，生产者会从给定的broker里查找到其他broker的信息
   - 一般提供两个broker的信息，一旦其中一个宕机，生产者仍然能够连接到集群上
 * key.serializer
   - broker希望收到的消息键和值都是字节数组，生产者接口允许使用参数化类型，因此可以把Java对象作为键和值发送给broker
   - 生产者需要知道如何把java对象转换成字节数组，key.serializer必须被设置为一个实现了Serializer接口的类，生产者使用这个把键对象序列化为字节数组
   - Kafka客户端默认提供了ByteArraySerializer、StringSerializer和IntegerSerilizer，如果使用常见的几种java对象类型，就没必要实现自己的系列化器
 * value.serializer
  - value.serializer指定的类会将消息的值序列化
  - 如果消息的键和值都是字符串，可以使用与key.serilizer一样的序列化器，如果键是整数而值是字符串，需要使用不同的序列化器
2.创建新的生产者，指定必要的属性，其他使用默认配置
  private Properties kafkaProps = new Properties();
  kafkaProps.put("bootstrap.servers","broker1:9092,broker2:9092");
  kafkaProps.put("key.serializer","org.apache.kafka.common.serializer.StringSerializer");
  kafkaProps.put("value.serializer","org.apache.kafka.common.serializer.StringSerializer");
  producer = new KafkaProducer<String,String>(kafkaProps);
3.发送消息有三种方式
  * 发送并忘记（fire-and-forget）
    把消息发送给服务器，并不关心消息是否正常到达，使用这种方式有时消息会丢失 
  * 同步发送
    使用send()方法发送消息，会返回一个Future对象，调用get()方法进行等待，可以知道消息是否发送成功
  * 异步发送
    调用send()方法，并指定一个回调函数，服务器在返回响应时调用该函数
```

- #### 发送消息到Kafka
```
1.同步发送消息
  ProducerRecord<String,String> record = new ProducerRecord<>("CustomerCountry","Precision Products","France");
  try{
      producer.send(record).get();
  }catch(Exception e){
      e.printStackTrace();
  }
  * producer.send()方法返回一个Future对象，调用Future对象的get()方法等待Kafka的响应，
    如果服务器返回错误，get()方法抛出异常，如果没有发生错误，返回RecordMateData对象，用它可以获取消息的偏移量
  * 如果在发送数据之前或者在发送过程中发生任何错误，如broker返回一个不允许重发消息的异常或者已经超过重发的次数，就会抛出异常
  * KafkaProducer可以被设置成自动重试，如果在多次重试后仍无法解决问题，应用程序会收到一个重试异常
2.异步发送消息
  * 尽管Kafka会把目标主题、分区信息和消息的偏移量发送回来，但对于发送端的应用程序并不是必需的，
    为了在异步发送消息的同时能够对异常情况进行处理，生产者提供了回调支持
  private class DemoProducerCallback implements Callback {
    @override
    public void onCompletion(RecordMateData recordMetaData,Exception e){
        // dosomething    
    }
  }        
  ProducerRecord<String,String> record = new ProducerRecord<>("CustomerCountry","Biomedical Materials","USA");
  producer.send(record,new DemoProducerCallback());
  * 使用回调，需要一个实现了org.apache.kafka.clients.producer.Callback接口的类，接口只有一个onCompletion方法
```

- #### 生产者的配置
```
1.acks
  * acks参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的
  * acks=0，生产者在成功写入消息之前不会等待任何来自服务器的响应，如果发生错误导致服务器没有收到消息，消息会丢失
  * acks=1，只要集群中的首领节点收到消息，生产者就会收到来自服务器的成功响应，
  如果消息无法到达首领节点(首领节点奔溃，新的首领还没有被选举出来)，生产者会收到错误响应，为避免数据丢失，生产者会重发消息，
  如果没有收到消息的节点成为新首领，消息还是会丢失
  * acks=all，只有当所以参与复制的节点全部收到消息时，生产者才会收到来自服务器的成功响应，
    这种模式是最安全的，可以保证不止一个服务器收到消息，就算服务器发生奔溃，整个集群仍然可以运行
2.buffer.memory
  * 该参数用来设置生产者内存缓存区的大小，生产者用它缓存要发送到服务的消息
  * 如果应用程序发送消息的速度超过发送到服务器的速度，会导致生产者空间不足，send方法要么被阻塞，要么抛出异常，取决于如何设置block.on.buffer.full参数
3.compression.type
  * 默认情况下，kafka消息发送时不会被压缩的
  * 该参数可以设置为snappy、gzip或lz4，指定了消息被发送给broker之前使用那一种压缩算法进行压缩
  * snappy压缩算法是由Google发明，它占用较少的CPU，却能提供较好的性能和相当客观的压缩比
  * gzip压缩算法一般会占用较多的CPU，但会提供更高的压缩比，所以网络宽带有限的情况，可以使用这种算法
4.reties
  * 生产者从服务器收到的错误可能只是临时的错误(如分区找不到首领)，reties参数的值决定了生产者可以重发消息的次数，如果达到这个次数，生产者会放弃重试返回错误
  * 默认情况下，生产者在每次重试之间等待100ms，可以通过retry.backoff.ms参数来改变这个时间间隔
5.batch.size
  * 当多个消息需要被发送到同一个分区时，生产者会把它们放到同一个批次
  * 改参数指定一个消息批次可以使用多少内存大小，按照字节数计算，当批次被填满，批次里的所有消息会被发送出去
  * 生产者并不一定会等到批次被填满才发送，半满的批次，甚至只包含一个消息的批次也有可能被发送
6.linger.ms
  * 该参数指定了生产者在发送批次之前等待更多消息加入批次的时间
  * kafkaProducer会在批次填满或linger.ms达到上限时把批次发送出去
  * 默认情况下，只要有可用线程，生产者就会把消息发送出去，就算批次里只有一个消息
  * 把linger.ms设置成比0大的数，让生产者在发送批次之前等待一会，使更多的消息加入到这个批次，这样虽然会增加延时，但也会提升吞吐量
7.client.id
  改参数可以是任意的字符串，服务器用来识别消息的来源，还可以用在日志和配额指标里
8.max.in.flight.requests.per.connection
  * 改参数指定了生产者在收到服务器响应之前可以发送多少个消息
  * 它的值越高就会占用越多的内存，不过也会提升吞吐量
  * 设置为1可以保证消息是按照发送的顺序写入服务器的，即使发生了重试
9.timeout.ms.request.timeout.ms和metadata.fetch.timeout.ms
  * timeout.ms：指定了broker等待同步副本返回消息确认的时间，与acks的配置相匹配
  * request.timeout.ms：指定了生产者在发送数据之前等待服务器响应的时间
  * fetch.timeout.ms：指定了生产者获取元数据时等待服务器返回响应的时间，如果等待响应超时，生产者要么重试发送数据，要么返回错误
10.max.block.ms
  * 该参数指定了在调用send()方法或使用partitionsFor()方法获取元数据时生产者阻塞时间
  * 在阻塞时间达到max.block.ms时，生产者会抛出异常
11.max.request.size
  * 该参数用于控制生产者发送的请求大小，可以指能发送的单个消息的最大值，也可以指单个请求里所有消息总的大小
  * broker对可接收的消息最大值也有自己的限制(message.max.bytes)，两边的配置最好可以匹配，避免生产者发送的消息被broker拒绝
12.receive.buffer.bytes和send.buffer.bytes
  * 两个参数分别指定了TCP scoket接收和发送数据包的缓冲区大小
  * 如果被设置为-1，使用操作系统的默认值
```

- #### 顺序保证
```
1.Kafka可以保证同一个分区里的消息是有序的，如果生产者按照一定的顺序发送消息，broker就会按照这个顺序把写入分区，消费者也会按照同样的顺序读取
2.如果reties设置为非0整数，同时把max.in.flight.requests.per.connection设置为比1大的数，
  当第一个批次消息写入失败，而第二个批次写入成功，broker会重试写入第一个批次，那么两个批次的顺序就会反过来
3.在对消息的顺序有严格要求的情况下，也不建议把reties设置为0，可以把max.in.flight.requests.per.connection设置为1  
```

- #### Avro序列化器
```
1.Apache Avro是一种与编程语言无关的序列化格式，Doug Cutting创建了这个项目，目的是提供一种共享数据文件的方式
2.Avro数据通过与语言无关的schema来定义，schema通过JSON来描述，数据被序列化成二进制文件或JSON文件
3.Avro在读写文件时需要用到schema，schema一般会被内嵌在数据文件中
4.负责写消息的应用程序使用了新的schema，负责读消息的应用程序可以继续处理消息而无需做任何改动
5.读取记录时需要用到整个schema，如果每条Kafka记录都嵌入schema，会让记录成倍的增加，遵从通用的结构模式使用schema注册表来达到目的
6.schema注册表并不属于Kafka，已有一些开源的schema注册表来实现，如Confluent Schema Registry
7.把所有写入数据需要用到的schema保存在注册表中，然后在记录里引用schema的标识符
8.序列化器和反序列化器分别负责处理schema的注册和拉取，Avro序列化器的使用方法与其他序列化器是一样的
9.使用Avro序列化器的实例：
  Properties props = new Properties();
  props.put("bootstrap.servers","localhost:9092");
  props.put("key.serializer","io.confluent.kafka.serializers.KafkaAvroSerializer");
  props.put("value.serializer","io.confluent.kafka.serializers.KafkaAvroSerializer");
  // schema.registry.url是一个新的参数，指向schema的存储位置
  props.put("schema.registry.url",schemaUrl);
  
  String topic ="customerContacts";
  Customer customer = CustomerGenerator.getNext();
  // 实例化ProducerRecord对象，并指定Customer为值的类型
  ProducerRecord<String,Customer> record = new ProducerRecord<>(topic,customer.getId(),customer);
  
  Producer<String,Customer> producer = new KafkaProducer<String,Customer>(props);
  // 把Customer对象作为记录发送，KafkaAvroSerializer会处理剩下的事情
  producer.send(record);
```

- #### 分区
```
1.Kafka的消息结构是一个个键值对，ProducerRecord对象包含消息的目标主题、键和值，键也可以被设置为null，大多数应用程序都会用到键
2.键有两个用途：
  * 可以作为消息的附件信息
  * 也可以用来决定消息被写到那个分区
  拥有相同键的消息将被写到同一个分区
3.创建包含键值对的ProducerRecord对象
  ProducerRecord<Integer,String> record = new ProducerRecord<>("CustomerCountry","Laboratory Equipment","USA");
  创建键为null的消息，不指定键就可以
  ProducerRecord<Integer,String> record = new ProducerRecord<>("CustomerCountry","USA");
4.如果键为null，并且使用默认的分区器，那个记录会被随机的发送到主题内各个可用的分区上，分区器使用轮询(Round Robin)算法讲消息均衡的分布到各个分区上
5.如果键不为空，并且采用默认的分区器，Kafka会对键进行散列，然后根据散列值把消息映射到特定的分区上，同一个键总是被映射到同一个分区上，
  原因是在映射时，会使用主题所有的分区，而不仅仅是可用的分区
6.只有在不改变主题分区数量的情况下，键与分区之间的映射才能保持不变，如果主题增加了分区，新的记录就可能被写到其他分区上
7.实现自定义分区策略：
  * 默认分区器是使用次数最多的分区器，除了使用散列分区之外，有时候需要对数据进行不一样的分区
  * 实现自定义分区策略需要实现Partitioner接口，并重写partition方法即可
```